from numbers import Number
from statistics import mean
from scipy import ndimage
import pandas as pd
import numpy as np
from odin.classes import AnalyzerClassification, Metrics, Curves
from odin.classes.dataset_cams import DatasetCAMs
from odin.classes.strings import err_type, err_value
from odin.utils import get_root_logger
from odin.utils.draw_utils import plot_cams

logger = get_root_logger()


class AnalyzerCAMs(AnalyzerClassification):

    _valid_cams_metrics = [Metrics.CAM_BBOX_COVERAGE, Metrics.CAM_COMPONENT_IOU, Metrics.CAM_GLOBAL_IOU,
                           Metrics.CAM_IRRELEVANT_ATTENTION]

    def __init__(self,
                 classifier_name,
                 dataset,
                 cam_thresh=0.5,
                 cam_coverage_thresh=0.2,
                 result_saving_path='./results/',
                 use_normalization=False,
                 norm_factor_categories=None,
                 norm_factors_properties=None,
                 conf_thresh=None,
                 metric=Metrics.CAM_GLOBAL_IOU,
                 save_graphs_as_png=True):
        """
        The AnalyzerCAMs class can be used to diagnose Class Activation Maps generated by classification models.

        Parameters
        ----------
        classifier_name: str
            Name of the classifier. It is used as folder to save results.
        dataset: DatasetCAMs
            Dataset used to perform the analysis.
        cam_thresh: Number, optional
            CAM confidence threshold. All the pixels with a confidence value less than the threshold are ignored. (default is 0.5)
        cam_coverage_thresh: Number, optional
            CAM coverage threshold. All the CAMs with a iou less than the threshold do not cover the annotation. (default is 0.5)
        result_saving_path: str, optional
            Path used to save results. (default is './results/')
        use_normalization: bool, optional
            Indicates whether normalisation should be used. (default is False)
        norm_factor_categories: float, optional
            Normalisation factor for the categories. If not specified, the default value is 1/number_of_categories. (default is None)
        norm_factors_properties: list of pair, optional
            Normalization factor for the properties. Each pair (property_name, value) specifies the normalisation factor to be applied to a specific property. If not specified, for each property the default value is 1/number_of_property_values. (default is None)
        conf_thresh: float, optional
            Confidence threshold. All the predictions with a confidence value less than the threshold are ignored. If not specified, the default value is 0.5. (default is None)
        metric: Metrics, optional
            The evaluation metric that will be used as default.<br>(default is Metrics.Metrics.CAM_GLOBAL_IOU)
        save_graphs_as_png: bool, optional
            Indicates whether plots should be saved as .png images. (default is True)
        """
        if type(dataset) is not DatasetCAMs:
            raise TypeError(f"Invalid dataset type: {type(dataset)}. Use DatasetCAMs.")

        if not isinstance(cam_thresh, Number):
            raise TypeError(err_type.format("cam_thresh"))
        elif cam_thresh < 0 or cam_thresh > 1:
            raise ValueError(err_value.format("cam_thresh", "0 <= x >= 1"))

        if not isinstance(cam_coverage_thresh, Number):
            raise TypeError(err_type.format("cam_coverage_thresh"))
        elif cam_coverage_thresh < 0 or cam_coverage_thresh > 1:
            raise ValueError(err_value.format("cam_coverage_thresh", "0 <= x >= 1"))

        if classifier_name not in dataset.cams:
            loaded_models = list(dataset.cams.keys())
            if len(loaded_models) == 1 and "default_model" in loaded_models:
                dataset.cams[classifier_name] = dataset.cams["default_model"]
                del dataset.cams["default_model"]
            else:
                raise Exception("No CAMs available. Please make sure to load the CAMs for {} model".format(classifier_name))

        self.cam_thresh = cam_thresh
        self.cam_coverage_thresh = cam_coverage_thresh

        if not dataset.proposals or classifier_name not in dataset.proposals:
            self._valid_metrics = []
            self._valid_curves = []

        self.__allow_class_analyses = (dataset.proposals and classifier_name in dataset.proposals)
        self.__SAVE_PNG_GRAPHS = save_graphs_as_png
        super().__init__(classifier_name, dataset, result_saving_path, use_normalization, norm_factor_categories,
                         norm_factors_properties, conf_thresh, metric, save_graphs_as_png)

    def analyze_property(self, property_name, possible_values=None, show=True, metric=None, split_by="meta-annotations", sort=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_property(property_name, possible_values, show, metric, split_by, sort)

    def analyze_properties(self, properties=None, metric=None, split_by="meta-annotations", show=True, sort=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_properties(properties, metric, split_by, show, sort)

    def analyze_sensitivity_impact_of_properties(self, properties=None, metric=None, show=True, sort=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_sensitivity_impact_of_properties(properties, metric, show, sort)

    def analyze_reliability(self, num_bins=10, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_reliability(num_bins, show)

    def analyze_reliability_for_categories(self, categories=None, num_bins=10, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_reliability(num_bins, show)

    def analyze_false_negative_errors_for_category(self, category, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_false_negative_errors_for_category(category, show)

    def analyze_false_positive_errors_for_category(self, category, metric=None, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_false_positive_errors_for_category(category, metric, show)

    def analyze_false_positive_trend_for_category(self, category, include_correct_predictions=True, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_false_positive_trend_for_category(category, include_correct_predictions, show)

    def show_confusion_matrix(self, categories=None, properties_names=None, properties_values=None, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_confusion_matrix(categories, properties_names, properties_values)

    def analyze_top1_top5_error(self, properties=None, metric=Metrics.ERROR_RATE, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_top1_top5_error(properties, metric, show)

    def base_report(self, metrics=None, categories=None, properties=None, show_categories=True, show_properties=True, include_reliability=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        return super().base_report(metrics, categories, properties, show_categories, show_properties, include_reliability)

    def analyze_false_negative_errors(self, categories=None, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_false_negative_errors(categories, show)

    def analyze_false_positive_errors(self, categories=None, metric=None, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_false_positive_errors(categories, metric, show)

    def analyze_curve(self, curve=Curves.PRECISION_RECALL_CURVE, average="macro", show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_curve(curve, average, show)

    def analyze_curve_for_categories(self, categories=None, curve=Curves.PRECISION_RECALL_CURVE, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_curve_for_categories(categories, curve, show)

    def show_true_positive_distribution(self, categories=None, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_true_positive_distribution(categories, show)

    def show_false_negative_distribution(self, categories=None, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_false_negative_distribution(categories, show)

    def show_false_positive_distribution(self, categories=None, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_false_positive_distribution(categories, show)

    def show_true_negative_distribution(self, categories=None, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_true_negative_distribution(categories, show)

    def show_true_negative_distribution_for_categories_for_property(self, property_name, property_values=None, categories=None, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_true_negative_distribution_for_categories_for_property(property_name, property_values, categories, show)

    def show_false_positive_distribution_for_categories_for_property(self, property_name, property_values=None, categories=None, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_false_positive_distribution_for_categories_for_property(property_name, property_values, categories, show)

    def show_true_positive_distribution_for_categories_for_property(self, property_name, property_values=None, categories=None, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_true_positive_distribution_for_categories_for_property(property_name, property_values, categories, show)

    def show_false_negative_distribution_for_categories_for_property(self, property_name, property_values=None, categories=None, show=True):
        if not self.__allow_class_analyses:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_false_negative_distribution_for_categories_for_property(property_name, property_values, categories, show)

    def analyze_cams_for_categories(self, categories=None, metric=None, show=True):
        """
        It provides a per-category analysis of the model CAMs at different thresholds.

        Parameters
        ----------
        categories: list, optional
            List of categories to be included in the analysis. If not specified, all the categories are included. (default is None)
        metric: Metrics, optional
            Evaluation metric used for the analysis. If not specified, the default one is used. (default is None)
        show: bool, optional
            Indicates whether the plot should be shown or not. If False, returns the results as dict. (default is True)

        """
        if not self._valid_cams_metrics:
            logger.error("Analysis not supported. Please be sure to load CAMs before instantiating the analyzer.")

        if categories is None:
            categories = self.dataset.get_categories_names()
        elif not self.dataset.are_valid_categories(categories):
            return -1

        if metric is None:
            metric = self.metric
        elif not self._is_valid_metric_cams(metric):
            logger.error(err_value.format("metric", self._valid_cams_metrics))
            return -1

        if not isinstance(show, bool):
            logger.error(err_type.format("show"))
            return -1

        thresholds = np.arange(0, 1.001, 0.05).round(3)
        results, display_names = {}, {}
        for category in categories:
            cat_id = self.dataset.get_category_id_from_name(category)
            res = self._analyze_threshold_for_category(cat_id, metric, thresholds)
            results[category] = {"threshold": thresholds,
                                 "metric_values": res}
            display_names[category] = self.dataset.get_display_name_of_category(category)

        if not show:
            return results

        plot_cams(results, metric, display_names, self.__SAVE_PNG_GRAPHS, self.result_saving_path)

    def analyze_cams(self, metric=None, show=True):
        """
        It provides the analysis of the model CAMs at different thresholds for the entire data set.

        Parameters
        ----------
        metric: Metrics, optional
            Evaluation metric used for the analysis. If not specified, the default one is used. (default is None)
        show: bool, optional
            Indicates whether the plot should be shown or not. If False, returns the results as dict. (default is True)

        """

        if not self._valid_cams_metrics:
            logger.error("Analysis not supported. Please be sure to load CAMs before instantiating the analyzer.")

        if metric is None:
            metric = self.metric
        elif not isinstance(metric, Metrics):
            logger.error(err_type.format("metric"))
            return -1
        elif not self._is_valid_metric_cams(metric):
            logger.error(err_value.format("metric", self._valid_cams_metrics))
            return -1

        if not isinstance(show, bool):
            logger.error(err_type.format("show"))
            return -1

        thresholds = np.arange(0, 1.001, 0.05).round(3)
        results = []
        for category in self.dataset.get_categories_names():
            cat_id = self.dataset.get_category_id_from_name(category)
            results.append(self._analyze_threshold_for_category(cat_id, metric, thresholds))

        result = {"overall": {"threshold": thresholds,
                              "metric_values": np.mean(np.array(results), axis=0)}}

        if not show:
            return result

        plot_cams(result, metric, {"overall": "Overall"}, self.__SAVE_PNG_GRAPHS, self.result_saving_path)

    def _analyze_threshold_for_category(self, category_id, metric, thresholds):
        """
        Calculates the evaluation metric specified for a specific category at different threshold values
        Parameters
        ----------
        category_id: int
        metric: Metrics
        thresholds: list

        Returns
        -------
        results
        """
        gt = self.dataset.get_gt_masks_for_category(category_id)
        cams = self.dataset.get_cams_for_images_ids_and_category(gt["image_id"].values, category_id, self._model_name)
        results = []
        for t in thresholds:
            results.append(self._compute_cams_metric(gt, cams, metric, t))
        return results

    def _calculate_metric_for_images_for_category(self, category_id, metric, images_ids=None):
        """
        Calculates the evaluation metric specified for a specific category on a specific images subset
        Parameters
        ----------
        category_id: int
            Category to be evaluated
        metric: Metrics
        threshold: float
        images_ids: list, optional
            Subset of images to be included in the analysis. If None, include all (default is None)

        Returns
        -------
        metric score
        """
        gt = self.dataset.get_gt_masks_for_category(category_id) if images_ids is None \
            else self.dataset.get_gt_masks_for_images_ids_and_category(images_ids, category_id)
        cams = self.dataset.get_cams_for_images_ids_and_category(gt["image_id"].values, category_id, self._model_name)
        return self._compute_cams_metric(gt, cams, metric, self.cam_thresh)

    def _compute_cams_metric(self, gt, cams, metric, threshold):
        """
        Evaluates the performance based on the metric specified
        Parameters
        ----------
        gt: DataFrame
        cams: DataFrame
        metric: Metrics
        threshold: float

        Returns
        -------
        metric score
        """
        if metric == Metrics.CAM_GLOBAL_IOU:
            return self._calculate_global_iou(gt, cams, threshold)
        if metric == Metrics.CAM_IRRELEVANT_ATTENTION:
            return self._calculate_irrelevant_attention(gt, cams, threshold)
        if metric == Metrics.CAM_BBOX_COVERAGE:
            return self._calculate_mask_coverage(gt, cams, threshold)
        if metric == Metrics.CAM_COMPONENT_IOU:
            return self._calculate_components_iou(gt, cams, threshold)
        raise Exception(f"Invalid metric: {metric}")

    def _calculate_global_iou(self, gt, props, threshold):
        """
        Calculates the Global IoU on the specific subset
        Parameters
        ----------
        gt: DataFrame
        props: DataFrame
        threshold: float

        Returns
        -------
        gIoU
        """
        matching = pd.merge(gt, props, how="left", on="image_id")
        result = matching.apply(lambda x: self.__calc_global_metrics(x["cam"], x["global_mask"], threshold),
                                axis=1)
        if result.empty:
            return 0
        gIoU = np.sum(np.array(list(result))[:, 0]) / len(gt.index)
        return gIoU

    def _calculate_irrelevant_attention(self, gt, props, threshold):
        """
        Calculates the Irrelevant Attention on the specific subset
        Parameters
        ----------
        gt: DataFrame
        props: DataFrame
        threshold: float

        Returns
        -------
        irrelevant_attention
        """
        matching = pd.merge(gt, props, how="left", on="image_id")
        result = matching.apply(lambda x: self.__calc_global_metrics(x["cam"], x["global_mask"], threshold),
                                axis=1)
        if result.empty:
            return 0
        irrelevant_attention = np.sum(np.array(list(result))[:, 1]) / len(gt.index)
        return irrelevant_attention

    def _calculate_mask_coverage(self, gt, props, threshold):
        """
        Calculates the Coverage on the specific subset
        Parameters
        ----------
        gt: DataFrame
        props: DataFrame
        threshold: float

        Returns
        -------
        bbox_coverage
        """

        matching = pd.merge(gt, props, how="left", on="image_id")
        result = matching.apply(lambda x: self.__gt_mask_coverage(x["cam"], x["masks"], threshold,
                                                                  self.cam_coverage_thresh), axis=1)
        if result.empty:
            return 0
        bbox_coverage = mean(result)
        return bbox_coverage

    def _calculate_components_iou(self, gt, props, threshold):
        """
        Calculates the Component IoU on the specific subset
        Parameters
        ----------
        gt: DataFrame
        props: DataFrame
        threshold: float

        Returns
        -------
        cIoU
        """

        matching = pd.merge(gt, props, how="left", on="image_id")
        result = matching.apply(lambda x: self.__calc_components_iou(x["cam"], x["masks"], threshold), axis=1)
        if result.empty:
            return 0
        cIoU = np.sum(result) / len(gt.index)
        return cIoU

    def __gt_mask_coverage(self, cam, masks, THRESHOLD, threshold_coverage):
        """
        Calculates the coverage evaluation metric
        Parameters
        ----------
        cam:
            CAM
        masks:
            Gt component masks
        THRESHOLD: float
        threshold_coverage: float

        Returns
        -------
        coverage_percentage
        """
        positive_cam = np.where(cam >= THRESHOLD, 1, 0) if THRESHOLD > 0 else np.where(cam > THRESHOLD, 1, 0)
        counter = 0
        coverages = []
        for mask in masks:
            int_cam = np.bitwise_and(mask, positive_cam).sum()
            coverage = int_cam / (mask.sum())
            if coverage > threshold_coverage:
                counter += 1
            coverages.append(coverage)

        coverage_percentage = counter / len(masks)
        return coverage_percentage

    def __calc_global_metrics(self, cam, annotation_mask, THRESHOLD):
        """
        Calculates the Global Intersection Over Union and the Irrelevant Attention evaluation metrics
        Parameters
        ----------
        cam:
            CAM
        annotation_mask:
            Gt global mask
        THRESHOLD: float

        Returns
        -------
            global_IoU, irrelevant_attention
        """
        # Trasform cam in mask having values == 0 or 1 based on threshold
        positive_cam = np.where(cam >= THRESHOLD, 1, 0) if THRESHOLD > 0 else np.where(cam > THRESHOLD, 1, 0)
        negative_cam = np.logical_not(positive_cam)
        negative_annotation_mask = np.logical_not(annotation_mask)

        # Calc number of pixels annotated
        total_annotation = annotation_mask.sum()
        # Validation
        assert total_annotation + negative_annotation_mask.sum() == annotation_mask.shape[0] * annotation_mask.shape[1]

        # Calc number of pixels annotated & with cam
        annotation_with_cams = np.bitwise_and(annotation_mask, positive_cam).sum()

        # Calc number of pixels annotated & without cam
        annotation_without_cams = np.bitwise_and(annotation_mask, negative_cam).sum()
        # Validation
        assert total_annotation == annotation_with_cams + annotation_without_cams

        # Calc number of pixels with cams
        total_cams = positive_cam.sum()

        # Calc number of pixels with cams & without annotation
        cams_without_annotation = np.bitwise_and(positive_cam, negative_annotation_mask).sum()
        # Validation
        assert total_cams == annotation_with_cams + cams_without_annotation

        global_IoU = annotation_with_cams / (annotation_with_cams + annotation_without_cams + cams_without_annotation)

        # Handle division by zero if no cams produced over the current THRESHOLD
        if total_cams != 0:
            irrelevant_attention = cams_without_annotation / total_cams
        else:
            irrelevant_attention = 0

        return global_IoU, irrelevant_attention

    def __calc_components_iou(self, cam, masks, THRESHOLD):
        """
        Calculates the Component Intersection Over Union
        Parameters
        ----------
        cam
            CAM
        masks
            Gt component masks
        THRESHOLD: float

        Returns
        -------
        iou

        """
        iou = 0
        positive_cam = np.where(cam >= THRESHOLD, 1, 0) if THRESHOLD > 0 else np.where(cam > THRESHOLD, 1, 0)
        # get connected components in a matrix component_cam with values
        # for each connected component == an increasing int starting from 1
        # nb = number of different components
        # 2 elements are connected if they are adiacent by row, col or diagonal
        component_cam, comp_number = ndimage.label(positive_cam)
        component_cams = []
        for i in range(1, comp_number + 1):
            component_cams.append(np.where(component_cam == i, 1, 0))

        # Calc IoU for each ground truth (bbox/segmentation)
        # and the connected components that intersect it
        for mask in masks:

            negative_mask = np.logical_not(mask)
            tot_intersections = 0
            union = mask.sum()
            for n_comp, component in enumerate(component_cams):
                intersection = np.bitwise_and(component, mask).sum()

                if intersection > 0:
                    tot_intersections += intersection
                    union += np.bitwise_and(component, negative_mask).sum()
            iou += tot_intersections / union
        # Average over all the ground truths
        iou = iou / len(masks)
        return iou

    def _is_valid_metric_cams(self, metric):
        """
        Checks if the metric is valid for CAMs evaluation
        Parameters
        ----------
        metric: Metrics

        Returns
        -------
        bool
        """
        return metric in self._valid_cams_metrics