from numbers import Number

from odin.classes import ComparatorClassification, Metrics, DatasetCAMs, AnnotationType, AnalyzerCAMs, Curves
from odin.classes.strings import err_type, err_value
from odin.utils import get_root_logger
from odin.utils.draw_utils import plot_cams

logger = get_root_logger()


class ComparatorCAMs(ComparatorClassification):

    __valid_cams_metrics = [Metrics.CAM_BBOX_COVERAGE, Metrics.CAM_COMPONENT_IOU, Metrics.CAM_GLOBAL_IOU,
                            Metrics.CAM_IRRELEVANT_ATTENTION]

    def __init__(self,
                 dataset_gt_param,
                 task_type,
                 multiple_proposals_path,
                 cam_thresh=0.5,
                 cam_coverage_thresh=0.2,
                 annotation_type=AnnotationType.BBOX,
                 result_saving_path='./results/',
                 properties_file='properties.json',
                 use_normalization=False,
                 norm_factor_categories=None,
                 norm_factors_properties=None,
                 conf_thresh=None,
                 metric=Metrics.CAM_GLOBAL_IOU,
                 similar_classes=None,
                 load_properties=True,
                 match_on_filename=False,
                 save_graph_as_png=True
                 ):
        """
        The ComparatorCAMs class can be used to compare Class Activation Maps generated by different classification models.

        Parameters
        ----------
        dataset_gt_param: str
            Path of the ground truth .json file.
        task_type: TaskType
            Problem task type. It can be: TaskType.CLASSIFICATION_BINARY, TaskType.CLASSIFICATION_SINGLE_LABEL, TaskType.CLASSIFICATION_MULTI_LABEL.
        multiple_proposals_path: list of list
            List of the models proposals path. For each model, it must be specified the model name and the model cams path directory; optionally can be specified also the predictions_path and the default confidence threshold. Example: multiple_proposals_path = [['model_a', ['model_a_cam_path', 'model_a_path'], 0.65], ['model_b', ['model_b_cam_path', 'model_b_path'], 0.45]]
        cam_thresh: float, optional
            CAM confidence threshold. All the pixels with a confidence value less than the threshold are ignored. (default is 0.5)
        cam_coverage_thresh: float, optional
            CAM coverage threshold. All the CAMs with a iou less than the threshold do not cover the annotation. (default is 0.5)
        annotation_type: AnnotationType, optional
            Indicates whether the annotation is a bounding box (AnnotationType.BBOX) or a segmentation mask (AnnotationType.SEGMENTATION) (default is AnnotationType.BBOX)
        result_saving_path: str, optional
            Path used to save results. (default is './results/')
        properties_file: str, optional
            The name of the file used to store the names of and values of the properties and the names of the categories. (default is 'properties.json')
        use_normalization: bool, optional
            Indicates whether normalisation should be used. (default is False)
        norm_factor_categories: float, optional
            Normalisation factor for the categories. If not specified, the default value is 1/number_of_categories. (default is None)
        norm_factors_properties: list of pairs, optional
            Normalization factor for the properties. Each pair (property_name, value) specifies the normalisation factor to be applied to a specific property. If not specified, for each property the default value is 1/number_of_property_values. (default is None)
        conf_thresh: float, optional
            Confidence threshold. All the predictions with a confidence value less than the threshold are ignored. If not specified, for single-label classification problems, the default value is 0. If not specified, for binary and multi-label classification problems the default value is 0.5. (default is None)
        metric: Metrics, optional
            The evaluation metric that will be used as default. (default is Metrics.CAM_GLOBAL_IOU)
        similar_classes: list of list, optional
            List of groups of ids of categories which are similar to each other. (default is None)
        match_on_filename: bool, optional
            Indicates whether the predictions refer to the ground truth by file_name (set to True) or by id (set to False). (default is False)
        save_graph_as_png: bool, optional
            Indicates whether plots should be saved as .png images. (default is True)
        """

        if not isinstance(multiple_proposals_path, list) or not all(isinstance(v[0], str) and (isinstance(v[1], str) or isinstance(v[1], list)) and len(v) > 1 for v in multiple_proposals_path):
            raise TypeError(err_type.format("multiple_proposals_path"))

        cams_paths, proposals_paths = [], []
        for p in multiple_proposals_path:
            cams_paths.append((p[0], p[1][0]))
            if len(p[1]) > 1:
                proposals_paths.append((p[0], p[1][1]))
        if len(proposals_paths) == 0:
            proposals_paths = None

        self._default_dataset = DatasetCAMs(dataset_gt_param,
                                            task_type,
                                            proposals_paths=proposals_paths,
                                            cams_paths=cams_paths,
                                            annotation_type=annotation_type,
                                            result_saving_path=result_saving_path,
                                            properties_file=properties_file,
                                            similar_classes=similar_classes,
                                            for_analysis=False,
                                            match_on_filename=match_on_filename,
                                            save_graphs_as_png=save_graph_as_png)

        if not isinstance(cam_thresh, Number):
            raise TypeError(err_type.format("cam_thresh"))
        elif cam_thresh < 0 or cam_thresh > 1:
            raise ValueError(err_value.format("cam_thresh", "0 <= x >= 1"))

        if not isinstance(cam_coverage_thresh, Number):
            raise TypeError(err_type.format("cam_coverage_thresh"))
        elif cam_coverage_thresh < 0 or cam_coverage_thresh > 1:
            raise ValueError(err_value.format("cam_coverage_thresh", "0 <= x >= 1"))

        self.cam_thresh = cam_thresh
        self.cam_coverage_thresh = cam_coverage_thresh

        self.__allow_class_comparison = len(multiple_proposals_path[0][1]) > 1

        super().__init__(dataset_gt_param, task_type, multiple_proposals_path, result_saving_path, properties_file,
                         use_normalization, norm_factor_categories, norm_factors_properties, conf_thresh, metric,
                         similar_classes, load_properties, match_on_filename, save_graph_as_png)

    def _load_all_models_proposals(self, load_properties=True):
        """
        Loads the proposals of all the models
        """
        self._default_dataset.for_analysis = True

        if load_properties:
            self._default_dataset._set_analyses_with_properties_available()
            if self._default_dataset.proposals_paths is not None:
                self._default_dataset.load_proposals()
            self._default_dataset.load_cams()

        else:
            self._default_dataset.load(load_properties=False)

        for p in self.proposals_path:
            model_name = p[0]
            threshold = p[2] if len(p)>2 else self.conf_thresh
            tmp_analyzer = AnalyzerCAMs(model_name,
                                        self._default_dataset,
                                        self.cam_thresh,
                                        self.cam_coverage_thresh,
                                        use_normalization=self.use_normalization,
                                        norm_factor_categories=self.norm_factor_categories,
                                        norm_factors_properties=self.norm_factors_properties,
                                        conf_thresh=threshold,
                                        metric=self.metric,
                                        save_graphs_as_png=False)

            self.models[model_name] = {"dataset": self._default_dataset,
                                       "analyzer": tmp_analyzer}

        self._allow_analyses = True

    def analyze_cams_threshold(self, metric=None, show=True):
        """
        It compares the CAMs of the models at different threshold values.

        Parameters
        ----------
        metric: Metrics, optional
            Evaluation metric used for the analysis. If not specified, the default one is used. (default is None)
        show: bool, optional
            Indicates whether the plot should be shown or not. If False, returns the results as dict. (default is True)
        """
        if not self._allow_analyses:
            logger.error("Please select the properties first")
            return -1

        if metric is None:
            metric = self.metric
        elif not isinstance(metric, Metrics):
            logger.error(err_type.format("metric"))
            return -1
        elif metric not in self.__valid_cams_metrics:
            logger.error(err_value.format("metric", self.__valid_cams_metrics))
            return -1

        results, display_names = {}, {}
        for model in self.models:
            results[model] = self.models[model]["analyzer"].analyze_cams(metric=metric, show=False)["overall"]
            display_names[model] = model

        if not show:
            return results

        plot_cams(results, metric, display_names, self.save_graph_as_png, self.result_saving_path)

    def show_true_negative_distribution(self, categories=None, show=True):
        if not self.__allow_class_comparison:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_true_negative_distribution(categories, show)

    def analyze_property(self, property_name, possible_values=None, metric=None, show=True):
        if not self.__allow_class_comparison:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_property(property_name, possible_values, metric, show)

    def analyze_sensitivity_impact_of_properties(self, properties=None, metric=None, show=True):
        if not self.__allow_class_comparison:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_sensitivity_impact_of_properties(properties, metric, show)

    def analyze_curve(self, curve=Curves.PRECISION_RECALL_CURVE, average="macro", show=True):
        if not self.__allow_class_comparison:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_curve(curve, average, show)

    def analyze_curve_for_categories(self, categories=None, curve=Curves.PRECISION_RECALL_CURVE, show=True):
        if not self.__allow_class_comparison:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_curve_for_categories(categories, curve, show)

    def analyze_false_positive_errors(self, categories=None, metric=None, show=True):
        if not self.__allow_class_comparison:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().analyze_false_positive_errors(categories, metric, show)

    def show_true_positive_distribution(self, categories=None, show=True):
        if not self.__allow_class_comparison:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_true_positive_distribution(categories, show)

    def show_false_positive_distribution(self, categories=None, show=True):
        if not self.__allow_class_comparison:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_false_positive_distribution(categories, show)

    def show_false_negative_distribution(self, categories=None, show=True):
        if not self.__allow_class_comparison:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        super().show_false_negative_distribution(categories, show)

    def base_report(self, metrics=None, categories=None, properties=None, show_categories=True,
                    show_properties=True):
        if not self.__allow_class_comparison:
            logger.error("Analysis not supported. Please make sure to load the proposals to the dataset.")
            return -1
        return super().base_report(metrics, categories, properties, show_categories, show_properties)
