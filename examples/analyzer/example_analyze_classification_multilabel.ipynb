{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of model analysis for multi-class multi-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### package import and variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from odin.classes import TaskType, Metrics, Curves, CustomMetric, DatasetClassification, AnalyzerClassification\n",
    "\n",
    "# define the path of the GT .json file\n",
    "dataset_gt_param = \"../../test-data/classification-ml/gt_art.json\"\n",
    "\n",
    "# define the path of the folder that contains the predictions .txt files for each model\n",
    "path_to_detections = \"../../test-data/classification-ml/predictions\"\n",
    "# path_to_detections = [(\"Model_A\", \"../../test-data/classification-ml/predictions\")\n",
    "#                       (\"Model_B\", \"../../test-data/classification-ml/predictions\")]\n",
    "\n",
    "# define the classification task (CLASSIFICATION_BINARY, CLASSIFICATION_SINGLE_LABEL, CLASSIFICATION_MULTI_LABEL)\n",
    "classification_type = TaskType.CLASSIFICATION_MULTI_LABEL\n",
    "\n",
    "# [OPTIONAL] define groups of categories which are similar to each other (useful for the error analysis)\n",
    "similar_classes=[[1, 4, 7], [2, 6, 10], [5, 8], [3, 6, 9]]\n",
    "\n",
    "# [OPTIONAL] define the file_name for the meta-annotations\n",
    "properties_file = \"properties_art.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = DatasetClassification(dataset_gt_param, \n",
    "                                   classification_type, \n",
    "                                   proposals_paths=path_to_detections,\n",
    "                                   similar_classes=similar_classes,\n",
    "                                   properties_file=properties_file, \n",
    "                                   save_graphs_as_png=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload the properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_dataset.reload_properties(from_file=False)\n",
    "\n",
    "# If you modify directly the '[properties].json' file\n",
    "# my_dataset.reload_properties(from_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset.show_co_occurrence_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categories distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset.show_distribution_of_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properties distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_dataset.show_distribution_of_properties()\n",
    "\n",
    "\n",
    "# [OPTIONAL] define specific properties to be analyzed\n",
    "# properties_to_be_analyzed = ['characters', 'color']\n",
    "\n",
    "# my_dataset.show_distribution_of_properties(properties=properties_to_be_analyzed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_dataset.show_distribution_of_property('color')\n",
    "\n",
    "\n",
    "# [OPTIONAL] define property values to be included in the analysis\n",
    "# values = [\"wikiart\", \"gallerix\"]\n",
    "\n",
    "# my_dataset.show_distribution_of_property('source', property_values=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-category property distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset.show_distribution_of_property_for_categories('color')\n",
    "\n",
    "# [OPTIONAL] define the categories to be analyzed\n",
    "# categories = [\"Paul\", \"Peter\"]\n",
    "\n",
    "# [OPTIONAL] define property values to be included in the analysis\n",
    "# values = [\"wikiart\", \"gallerix\"]\n",
    "\n",
    "# my_dataset.show_distribution_of_property_for_categories('source', property_values=values, categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset.show_distribution_of_property_for_category('color', 'Paul')\n",
    "\n",
    "# [OPTIONAL] define property values to be included in the analysis\n",
    "# values = [\"wikiart\", \"gallerix\"]\n",
    "\n",
    "# my_dataset.show_distribution_of_property_for_category('source', 'Paul', property_values=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer = AnalyzerClassification('Model_A',  # N.B. the name must be the same of ones in the list of the proposals\n",
    "                                     my_dataset,\n",
    "                                     metric=Metrics.F1_SCORE,\n",
    "                                     save_graphs_as_png=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load custom display names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you change the display names of the categories in the properties file\n",
    "my_dataset.load_categories_display_names()\n",
    "\n",
    "# if you change the display names of the properties in the properties file\n",
    "my_dataset.load_properties_display_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance analysis based on meta-annotations\n",
    "\n",
    "The evaluation metric is computed by considering only ground truth subsets with a specific meta-annotation value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_analyzer.analyze_properties()\n",
    "\n",
    "# [OPTIONAL] define only specific properties to be analyzed\n",
    "# meta_annotations = ['color', 'source']\n",
    "\n",
    "# [OPTIONAL] define only specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# [OPTIONAL] set the evaluation metric to be used in the analysis\n",
    "# eval_metric = Metrics.PRECISION_SCORE\n",
    "\n",
    "# [OPTIONAL] set the split method (by categories or by meta-annotations)\n",
    "# split_by = \"categories\"\n",
    "\n",
    "# my_analyzer.analyze_properties(properties=meta_annotations, categories=categories, metric=eval_metric, split_by=split_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_analyzer.analyze_property(\"color\")\n",
    "\n",
    "# [OPTIONAL] define only specific values to be analyzed\n",
    "# meta_annotation_values = [\"rgb\"]\n",
    "\n",
    "# [OPTIONAL] define only specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# [OPTIONAL] set the evaluation metric to be used in the analysis\n",
    "# eval_metric = Metrics.PRECISION_SCORE\n",
    "\n",
    "# [OPTIONAL] set the split method (by categories or by meta-annotations)\n",
    "# split_by = \"categories\"\n",
    "\n",
    "# my_analyzer.analyze_property(\"color\", possible_values= meta_annotation_values, categories=categories, metric=eval_metric,  split_by=split_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.analyze_sensitivity_impact_of_properties()\n",
    "\n",
    "# [OPTIONAL] define specific meta-annotations to be included in the analysis\n",
    "# meta_annotations = ['characters', 'color']\n",
    "\n",
    "# [OPTIONAL] set the evaluation metric to be used in the analysis\n",
    "# eval_metric = Metrics.PRECISION_SCORE\n",
    "\n",
    "# my_analyzer.analyze_sensitivity_impact_of_properties(properties=meta_annotations, metric=eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False positives errors analysis\n",
    "For each category, FP errors are categorized into \"similar\", \"background\", and \"other\", and the impact of each type of error on the evaluation metric score is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_analyzer.analyze_false_positive_errors()\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# [OPTIONAL] set the evaluation metric to be used in the analysis\n",
    "# eval_metric = Metrics.PRECISION_SCORE\n",
    "\n",
    "# my_analyzer.analyze_false_positive_errors(categories=categories, metric=eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_analyzer.analyze_false_positive_errors_for_category('Paul', metric=Metrics.ACCURACY)\n",
    "\n",
    "# [OPTIONAL] set the evaluation metric to be used in the analysis\n",
    "# eval_metric = Metrics.PRECISION_SCORE\n",
    "\n",
    "# my_analyzer.analyze_false_positive_errors_for_category('Paul', metric=eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.analyze_false_positive_trend()\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Peter', 'Jerome']\n",
    "\n",
    "# [OPTIONAL] define whether the correct detections should be included\n",
    "# include_correct = False\n",
    "\n",
    "# my_analyzer.analyze_false_positive_trend(categories=categories, include_correct_predictions=include_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.analyze_false_positive_trend_for_category(\"Jerome\")\n",
    "\n",
    "# [OPTIONAL] define whether the correct detections should be included\n",
    "# include_correct = False\n",
    "\n",
    "# my_analyzer.analyze_false_positive_trend_for_category(\"Paul\", include_correct_predictions=include_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False negatives errors analysis\n",
    "For each category, FN errors are categorized into \"similar\" and \"other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_analyzer.analyze_false_negative_errors()\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# my_analyzer.analyze_false_negative_errors(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.analyze_false_negative_errors_for_category(\"Paul\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance analysis based on PR curve, ROC curve and F1 curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.analyze_curve()\n",
    "\n",
    "# [OPTIONAL] set the curve to be used in the analysis (default is PRECISION_RECALL_CURVE)\n",
    "# eval_curve = Curves.ROC_CURVE \n",
    "\n",
    "# [OPTIONAL] set the averaging method to be used in the analysis (default is \"macro\")\n",
    "# avg_method = \"micro\"\n",
    "\n",
    "# my_analyzer.analyze_curve(curve=eval_curve, average=avg_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.analyze_curve_for_categories()\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# [OPTIONAL] set the curve to be used in the analysis (default is PRECISION_RECALL_CURVE)\n",
    "# eval_curve = Curves.ROC_CURVE \n",
    "\n",
    "# my_analyzer.analyze_curve_for_categories(categories=categories, curve=eval_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.analyze_reliability()\n",
    "\n",
    "# [OPTIONAL] define the number of bins for the analysis (default is 10)\n",
    "# bins = 20\n",
    "# my_analyzer.analyze_reliability(num_bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_analyzer.analyze_reliability_for_categories()\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# [OPTIONAL] define the number of bins for the analysis (default is 10)\n",
    "# bins = 20\n",
    "\n",
    "# my_analyzer.analyze_reliability_for_categories(categories=categories, num_bins=bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP, FP, FN, TN analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.show_confusion_matrix()\n",
    "\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# my_analyzer.show_confusion_matrix(categories=categories)\n",
    "\n",
    "# [OPTIONAL] filter the output by meta-annotations values\n",
    "# meta_annotations = ['color', 'characters']\n",
    "# meta_annotations_values = [['rgb'], ['0-1', '5+']]\n",
    "\n",
    "# my_analyzer.show_confusion_matrix(properties_names=meta_annotations, properties_values=meta_annotations_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.show_true_positive_distribution()\n",
    "\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# my_analyzer.show_true_positive_distribution(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.show_true_negative_distribution()\n",
    "\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# my_analyzer.show_true_negative_distribution(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.show_false_negative_distribution()\n",
    "\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# my_analyzer.show_false_negative_distribution(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.show_false_positive_distribution()\n",
    "\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# my_analyzer.show_false_positive_distribution(categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per-property distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_analyzer.show_true_positive_distribution_for_categories_for_property(\"color\")\n",
    "\n",
    "# [OPTIONAL] define only specific values to be analyzed\n",
    "# meta_annotation_values = [\"rgb\"]\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# my_analyzer.show_true_positive_distribution_for_categories_for_property(\"color\", property_values=meta_annotation_values, categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.show_true_negative_distribution_for_categories_for_property(\"color\")\n",
    "\n",
    "# [OPTIONAL] define only specific values to be analyzed\n",
    "# meta_annotation_values = [\"rgb\"]\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# my_analyzer.show_true_negative_distribution_for_categories_for_property(\"color\", property_values=meta_annotation_values, categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.show_false_negative_distribution_for_categories_for_property(\"color\")\n",
    "\n",
    "# [OPTIONAL] define only specific values to be analyzed\n",
    "# meta_annotation_values = [\"rgb\"]\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# my_analyzer.show_false_negative_distribution_for_categories_for_property(\"color\", property_values=meta_annotation_values, categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.show_false_positive_distribution_for_categories_for_property(\"color\")\n",
    "\n",
    "# [OPTIONAL] define only specific values to be analyzed\n",
    "# meta_annotation_values = [\"rgb\"]\n",
    "\n",
    "# [OPTIONAL] define specific categories to be analyzed\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# my_analyzer.show_false_positive_distribution_for_categories_for_property(\"color\", property_values=meta_annotation_values, categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report of all the performance at all levels of granularity (overall, per-category, per-meta-annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_analyzer.base_report()\n",
    "\n",
    "\n",
    "# [OPTIONAL] define specific evaluation metrics to be included in the report\n",
    "# eval_metrics = [Metrics.ACCURACY, Metrics.F1_SCORE]\n",
    "\n",
    "# [OPTIONAL] define specific categories to be included in the report\n",
    "# categories = ['Paul', 'Peter']\n",
    "\n",
    "# [OPTIONAL] define specific meta-annotations to be included in the report\n",
    "# meta_annotations = ['characters', 'color']\n",
    "\n",
    "# [OPTIONAL] do not include categories and/or meta-annotations in the report\n",
    "# show_categories = False\n",
    "# show_meta_annotations = False\n",
    "\n",
    "# my_analyzer.base_report(metrics=eval_metrics, categories=categories, properties=meta_annotations, show_categories=show_categories, show_properties=show_meta_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use your custom evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SklearnF1(CustomMetric):\n",
    "    def evaluate_metric(self, gt, predictions, matching, is_micro_required=False):\n",
    "        predictions = np.array(predictions) >= 0.5\n",
    "        # when called from analysis\n",
    "        if not is_micro_required: #is_micro_required == false\n",
    "            return f1_score(gt, predictions, zero_division=0), None\n",
    "        #when called from base report for micro avg\n",
    "        else: #is_micro_required == true\n",
    "            return f1_score(gt, predictions, zero_division=0, average='micro'), None\n",
    "        \n",
    "class SklearnAP(CustomMetric):\n",
    "    def evaluate_metric(self, gt, predictions, matching, is_micro_required=False):\n",
    "        # when called from analysis\n",
    "        if not is_micro_required: #is_micro_required == false\n",
    "            return average_precision_score(gt, predictions), None\n",
    "        #when called from base report for micro avg\n",
    "        else: #is_micro_required == true\n",
    "            return average_precision_score(gt, predictions, average='micro'), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add custom metrics to analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_f1 = SklearnF1(\"sklearn F1\", is_single_threshold_metric=True)\n",
    "sklearn_ap = SklearnAP(\"sklearn AP\", is_single_threshold_metric=False)\n",
    "\n",
    "my_analyzer.add_custom_metric(sklearn_f1)\n",
    "my_analyzer.add_custom_metric(sklearn_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use your custom metrics for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CustomMetric** instances just created have been added to the _Metrics_ enum. In order to evaluate the model performances with your evaluation metric, use 'Metrics.MY_CUSTOM_METRIC_NAME', where 'MY_CUSTOM_METRIC_NAME' is the name of the custom metric created in uppercase and with all the white spaces replaced with the underscore.\n",
    "\n",
    "Example: \"sklearn F1\" -> Metrics.SKLEARN_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.analyze_property(\"color\", metric=Metrics.SKLEARN_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.analyze_sensitivity_impact_of_properties(metric=Metrics.SKLEARN_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analyzer.analyze_false_positive_errors(metric=Metrics.SKLEARN_AP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_analyzer.base_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
