{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from odin.classes import DatasetClassification, AnalyzerClassification, TaskType\n",
    "\n",
    "dataset_gt_param = \"../test-data/classification-ml/gt_art.json\"\n",
    "path_to_detections = \"../test-data/classification-ml/predictions\"\n",
    "\n",
    "classification_type = TaskType.CLASSIFICATION_MULTI_LABEL\n",
    "\n",
    "#properties_file: default = properties.json\n",
    "\n",
    "# DatasetClassification(dataset_gt_param, task_type, proposal_path=None, observations_set_name='test',\n",
    "#                 observations_abs_path=None, similar_classes=None, property_names=None, terminal_env=False,\n",
    "#                 properties_file=None, for_analysis=True, match_on_filename=False)\n",
    "\n",
    "my_dataset = DatasetClassification(dataset_gt_param, classification_type, proposal_path=path_to_detections, \n",
    "                                   similar_classes=[[1, 4, 7], [2, 6, 10], [5, 8], [3, 6, 9]], property_names=None, terminal_env=False, properties_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_normalization: default=True\n",
    "# conf_threshold: default=0.5\n",
    "# metric: default='average_precision_score'\n",
    "\n",
    "# help(AnalyzerClassification)\n",
    "\n",
    "my_analyzer = AnalyzerClassification('Test',  my_dataset, use_normalization=False,\n",
    "                 norm_factor_categories=None, norm_factors_properties=None, conf_thresh=None, metric='f1_score',\n",
    "                 save_graphs_as_png=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties: list of properties to include. If None include all\n",
    "my_analyzer.show_distribution_of_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_name: name of the property for the analysis\n",
    "# show_distribution_of_property(property_name)\n",
    "\n",
    "my_analyzer.show_distribution_of_property(property_name='color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties: list of properties to include in the analysis. If None, include all properties\n",
    "# metric: if None use the one specified in the ClassificationAnalyzer\n",
    "\n",
    "# analyze_properties(properties=None, metric=None)\n",
    "\n",
    "my_analyzer.analyze_properties(properties=['color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_name: name of the property to analyze\n",
    "# possible_values: list of the property values to include in the analysis\n",
    "# labels: list of the names of the properties to show. If None, show the default names\n",
    "# show: boolean, if True, show the plot\n",
    "# metric: if None use the one specified in the ClassificationAnalyzer\n",
    "\n",
    "# analyze_property(property_name, possible_values, labels=None, show=True, metric=None)\n",
    "\n",
    "my_analyzer.analyze_property(\"color\", [\"rgb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties: list of properties to include in the analysis. If None include all.\n",
    "# metric: if None use the one specified in the ClassificationAnalyzer\n",
    "\n",
    "# analyze_sensitivity_impact_of_properties(properties=None, metric=None)\n",
    "\n",
    "my_analyzer.analyze_sensitivity_impact_of_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories: list of categories to include. If None, include all\n",
    "# metric: if None use the one specified in the ClassificationAnalyzer\n",
    "\n",
    "# analyze_false_positive_errors(categories=None, metric=None)\n",
    "\n",
    "my_analyzer.analyze_false_positive_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories: list of categories to include. If None, include all\n",
    "\n",
    "# get_fp_error_distribution(categories=None)\n",
    "\n",
    "my_analyzer.get_fp_error_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories: list of categories to include. If None, include all\n",
    "# my_analyzer.get_tp_distribution(categories=None)\n",
    "\n",
    "my_analyzer.get_tp_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories: list of categories to include. If None, include all\n",
    "# my_analyzer.get_fn_distribution(categories=None)\n",
    "\n",
    "my_analyzer.get_tn_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories: list of categories to include. If None, include all\n",
    "# my_analyzer.get_tn_error_distribution(categories=None)\n",
    "\n",
    "my_analyzer.get_fn_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category: category name for the analysis\n",
    "# metric: if None use the one specified in the ClassificationAnalyzer\n",
    "\n",
    "# analyze_false_positive_error_for_category(category, categories, metric=None).\n",
    "\n",
    "my_analyzer.analyze_false_positive_error_for_category(\"Paul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories: list of categories to include. If none, include all\n",
    "# curve: name of the curve to plot (precision_recall_curve, f1_curve, roc_curve)\n",
    "\n",
    "# analyze_curve_for_categories(categories=None, curve='precision_recall_curve')\n",
    "\n",
    "my_analyzer.analyze_curve_for_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories: list of categories to include in the analysis. If none, include all\n",
    "# num_bins: number of bins the data are split in. Min=2, Max=50\n",
    "\n",
    "# analyze_reliability(categories=None, num_bins=10)\n",
    "\n",
    "my_analyzer.analyze_reliability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# categories: list of categories to include. If none, include all\n",
    "# properties_names: list of properties for the filter. Can be None.\n",
    "# properties_values: nested list of values to include for each properties names specified before. Can be None\n",
    "\n",
    "# analyze_confusion_matrix(categories=None, properties_names=None, properties_values=None)\n",
    "\n",
    "# my_analyzer.analyze_confusion_matrix(categories=[\"Paul\", \"Jerome\"], properties_names=['color'], \n",
    "#                                      properties_values=[['rgb']])\n",
    "my_analyzer.analyze_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics: list of metrics to include. If None, include the default\n",
    "# categories: list of categories to include. If none, include all\n",
    "# properties: list of properties to include. If none, include all\n",
    "# show_categories: boolean. If False and categories is None, do not show categories report\n",
    "# show_properties: boolean. If False and properties is None, do not show properties report\n",
    "\n",
    "# base_report(metrics=None, categories=None, properties=None, show_categories=True, \n",
    "#                  show_propertiest=True)\n",
    "\n",
    "# returns a Dataframe\n",
    "\n",
    "my_analyzer.base_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(my_analyzer.set_nomalization)\n",
    "# my_analyzer.set_normalization(False)\n",
    "\n",
    "# help(my_analyzer.set_confidence_threshold)\n",
    "# my_analyzer.set_confidence_threshold(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to define a custom metric"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "class AnalyzerCustom(AnalyzerClassification):\n",
    "    def _evaluation_metric(self, gt, detections, matching, is_micro_required):\n",
    "        detections = detections > 0.5\n",
    "        # when called from analysis / base report\n",
    "        if not is_micro_required: #is_micro_required == false\n",
    "            return f1_score(gt, detections), None\n",
    "        #when called from base report for micro avg\n",
    "        else: #is_micro_required == true\n",
    "            return f1_score(gt, detections, average='micro'), None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
